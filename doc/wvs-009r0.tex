\documentclass[11pt]{article}
\usepackage[fleqn]{amsmath}\textwidth 6.25in
\oddsidemargin -0.25in
%\evensidemargin -0.5in
\topmargin -0.5in
\textheight 9.00in

\begin{document}

%\tracingcommands=1
\newlength{\hW} % heading box width
%\settowidth{\hW}{\bf wvs-005}
\settowidth{\hW}{Page \pageref{lastpage}\ of \pageref{lastpage}}
\makeatletter
\def\@biblabel#1{#1.}
\newcommand{\ps@twolines}{%
  \renewcommand{\@oddhead}{%
    3 November 2000\hfill Page \thepage\ of \pageref{lastpage}\hfill{\bf
    wvs-009}}%
\renewcommand{\@evenhead}{}%
\renewcommand{\@oddfoot}{}%
\renewcommand{\@evenfoot}{}%
}%
\makeatother
\pagestyle{twolines}

\vspace{-10pt}
\begin{tabbing}
\phantom{References: }\= \\
To: \>Dave, Nathaniel\\
Subject: \>Changes in mathematical method for EOS MLS retrieval\\
From: \>Van Snyder, Fred Krogh\\
\end{tabbing}

\parindent 0pt \parskip 3pt
\vspace{-20pt}

\section{Introduction}

In wvs-002, the Newtonian iteration for inverting the radiative transfer
equation described in the \emph{EOS MLS Retrieval Process Algorithm
Theoretical Basis} document JPL D-16159 is shown in terms of a
least-squares problem, \emph{viz.}

\begin{equation}
\left [ \begin{array}{c} \mathbf{W}_1 \mathbf{K}_1 \\
                         \text{...} \\
                         \mathbf{W}_n \mathbf{K}_n \\
                         \mathbf{W_a} \\
                         \, \\
                         \lambda \mathbf{I}
         \end{array} \right ]
 [ \mathbf{x}^{(r+1)} - \mathbf{x}^{(r)} ] \;\simeq\;
\left [ \begin{array}{c} \mathbf{W}_1 \: [
                           \mathbf{y}_1 - \mathbf{f}_1(\mathbf{x}^{(r)}) ] \\
                         \text{...} \\
                         \mathbf{W}_n \: [
                           \mathbf{y}_i - \mathbf{f}_n(\mathbf{x}^{(r)}) ] \\
                           \mathbf{W_a}
                           \: [ \mathbf{a} - \mathbf{x}^{(r)} ] \\
                         \, \\
                         0
         \end{array} \right ] \;,
\end{equation}\label{orig}

or, more briefly, $\mathbf{A \, \delta x} \simeq \mathbf{b}$.  A
least-squares problem can be converted to a system of equations by
multiplying by $\mathbf{A^T}$.  The result $\mathbf{A^T A \, \delta x} =
\mathbf{A^T b}$ is called a \emph{system of normal equations}.

It is easier to reason mathematically about least-squares problems in the
form of normal equations, but there is some danger in actually forming
them: the condition number is squared.  The condition number is the ratio
of the norm of a matrix to the norm of its inverse.  It is a rough
measure of the loss of precision to be expected in the solution.  There
are methods to solve least-squares problems directly, that is, without
forming normal equations, but these methods require roughly twice as much
computational effort as is required to form normal equations, and then
solve them.  There is therefore some attraction to solving a
least-squares problem in normal equations form.  This can safely be done
if one knows a priori that the condition number is not large.

We have reason to believe that the condition number of the least-squares
problem in the Newtonian iteration for inverting the radiative transfer
equation is not large.  Therefore, we propose to solve it by forming
normal equations, as described in JPL D-16159.

\section{Forming the normal equations}

Observing that the matrices $\mathbf{W}_i$ and $\mathbf{W}_a$
are Cholesky factors of the inverses of the measurement and a priori
covariance matrices $\mathbf{S}_i^{-1}$ and $\mathbf{S}_a^{-1}$,
respectively, and denoting $\mathbf{A^T b}$ by $\mathbf{c}$, the system
of normal equations of the original least-squares problem (\ref{orig}) is

\begin{equation}
 \left ( \sum_{i=1}^n \mathbf{K}_i^\mathbf{T} \mathbf{S}_i^{-1} \mathbf{K}_i +
  \mathbf{S}_a^{-1} + \lambda^2 \mathbf{I} \right )\: \mathbf{\delta x} =
 \sum_{i=1}^n \mathbf{K}_i^\mathbf{T} \mathbf{S}_i^{-1}
   \: [ \mathbf{y}_i - \mathbf{f}_i(\mathbf{x}^{(r)}) ]
   + \mathbf{S}_a^{-1} [ \mathbf{a} - \mathbf{x}^{(r)} ] = \mathbf{c} \: .
\end{equation}\label{normal}

To reduce the storage requirements, this system of equations can be built
up a little bit at a time.  The process starts with the set of equations
$\mathbf{S}_a^{-1} \: \mathbf{\delta x} = \mathbf{S}_a^{-1} [ \mathbf{a}
- \mathbf{x}^{(r)} ]$ and adds equations $\mathbf{K}_i^\mathbf{T}
\mathbf{S}_i^{-1} \mathbf{K}_i \: \mathbf{\delta x} =
\mathbf{K}_i^\mathbf{T} \mathbf{S}_i^{-1} \: [ \mathbf{y}_i -
\mathbf{f}_i(\mathbf{x}^{(r)}) ]$ as they are formed by the forward
model.

After having done this, $\mathbf{c}$ is the gradient of the nonlinear
problem.  The nonlinear solver uses the gradient to compute $\lambda$. 
After $\lambda$ is computed, the equations $\lambda^2 \mathbf{I} \:
\mathbf{\delta x} = 0$ are added to the system.

\section{Methods to solve normal equations}

By the nature of their formation, normal equations are symmetric and
positive definite.  There is an attractive algorithm for solving
symmetric and positive definite systems of equations, known as Cholesky
decomposition.  Given a symmetric and positive definite matrix
$\mathbf{X}$, a Cholesky decomposition constructs an upper-triangular
matrix $\mathbf{U}$ such that $\mathbf{U^T U} = \mathbf{X}$.  In factored
form the normal equations are $\mathbf{A^T A \delta x} = \mathbf{U^T U \,
\delta x} = \mathbf{c}$.  Since $\mathbf{U}$ is triangular, this system
can easily be solved in two steps, first by solving $\mathbf{U^T y} =
\mathbf{c}$ for $\mathbf{y}$, and then solving $\mathbf{U \, \delta x} =
\mathbf{y}$ for $\mathbf{\delta x}$.

\section{Updating the normal equations}

The equations of the form $\lambda \mathbf{I} = 0$ in (\ref{orig}) arise
from Levenberg-Marquardt stabilization of the Newtonian iteration.  The
effect of a large $\lambda$ is to decrease the norm of the solution
$\mathbf{\delta x}$.  The nonlinear solver starts with a small $\lambda$,
which allows a large $\mathbf{\delta x}$, and increases it if necessary
so that $\mathbf{\delta x}$ is constrained to remain within a
predetermined \emph{trust region}. It may seem that in order to
accomodate changing $\lambda$ one would need to retain the (upper
triangle of the) normal equations $\mathbf{A^T A} \: \mathbf{\delta x} =
\mathbf{c}$ so that a revised system $( \mathbf{A^T A} + \delta \lambda^2
\mathbf{I} ) \: \mathbf{\delta x} = \mathbf{c}$ can be solved.  (It is
not a good idea to reconstruct the normal equations as $( \mathbf{U^T U}
+ \delta \lambda^2 \mathbf{I} ) \: \mathbf{\delta x} = \mathbf{c}$, as
this squares the condition number again.)  It turns out not to be
necessary.  Start with $\mathbf{U \delta x} = \mathbf{U^{-T} c} =
\mathbf{y}$ and add new rows $\delta \lambda \mathbf{I} \mathbf{\delta x}
= 0$ to form the least-squares problem

\begin{equation}
 \left [
  \begin{array}{c}\mathbf{U} \\ \delta \lambda \mathbf{I} \end{array}
 \right ] \mathbf{\delta x} \simeq
 \left [ \begin{array}{c}\mathbf{y} \\ 0 \end{array} \right ]
\end{equation}

for which the normal equations $( \mathbf{U^T U} + \delta \lambda^2
\mathbf{I} ) \: \mathbf{\delta x} = \mathbf{c}$ are the same as would be
obtained by replacing $\lambda^2$ by $\lambda^2 + \delta \lambda^2$ in
(\ref{normal}), or adding equations $\delta\lambda\mathbf{I}\delta x = 0$
to equation (\ref{orig}).  It is not the same as replacing $\lambda$ by
$\lambda + \delta\lambda$ in equation (\ref{orig}), and this is not what
is expected by the nonlinear solver.  Do not solve this system by
constructing normal equations, as this squares the condition number
again.  Instead, construct an orthogonal transformation $\mathbf{Q}$ so
that

\begin{equation}
 \left [ \begin{array}{c}\mathbf{\hat U} \\ 0 \end{array} \right ]
  \mathbf{\delta x} =
 \mathbf{Q}
 \left [
  \begin{array}{c}\mathbf{U} \\ \delta \lambda \mathbf{I} \end{array}
 \right ] \mathbf{\delta x} \simeq
 \mathbf{Q}
 \left [ \begin{array}{c}\mathbf{y} \\ 0 \end{array} \right ] =
 \left [ \begin{array}{c}\mathbf{\hat y} \\ \zeta \end{array} \right ]
\end{equation}\label{update}

and $\mathbf{\hat U}$ is triangular.  The vector $\zeta$ is not relevant
and is not computed.  This procedure does not square the condition
number.  One can compute $\mathbf{\hat y}$ while computing $\mathbf{\hat
U}$ by applying $\mathbf{Q}$ to $\mathbf{y}$, or notice

\begin{equation}
\begin{split}
 \mathbf{\hat U^T \hat U} \mathbf{\delta x} &=
  \left [ \mathbf{U^T} | \delta \lambda
   \mathbf{I} \right ] \mathbf{Q^T Q}
   \left [
  \begin{array}{c}\mathbf{U} \\ \delta \lambda \mathbf{I} \end{array}
 \right ] \mathbf{\delta x} =
 \left ( \mathbf{U^T U} + \delta \lambda^2 \mathbf{I} \right )
  \mathbf{\delta x} =\\
  & = \left [ \mathbf{U^T} | \delta \lambda \mathbf{I} \right ] \mathbf{Q^T Q}
 \left [ \begin{array}{c}\mathbf{y} \\ 0 \end{array} \right ] =
 \mathbf{U^T U^{-T} c} = \mathbf{c} \: .\\
\end{split}
\end{equation}

and solve $\mathbf{\hat U^T \hat y} = \mathbf{c}$ for $\mathbf{\hat y}$.
In either case, solve $\mathbf{\hat U \delta x} = \mathbf{\hat y}$ for
$\mathbf{\delta x}$.

The matrix $\mathbf{Q}$ can either be the product of Givens rotations or
Householder reflections.  The latter requires less computational effort
if the number of rows to be added is large, as it is in this case.

It requires more storage but less time to retain $\mathbf{A^T A}$,
replace it by $\mathbf{A^T A + \delta \lambda^2 I}$, and solve the
revised system by Cholesky decomposition.  It requires less storage but
more time to update the system as described by equation (\ref{update}). 
There should be no difference in stability.

It requires more storage but less time, and it may be more stable, to
retain $\mathbf{c}$ and solve $\mathbf{\hat U^T \hat y} = \mathbf{c}$. 
It requires less storage but more time, and it may be less stable, to
compute $\mathbf{\hat y}$ as described by equation (\ref{update}).  To
see that it may be less stable to compute $\mathbf{\hat y}$ as in
equation (\ref{update}), suppose the original problem were poorly
conditioned because of having too small a value for $\lambda$.  This may
result in $\mathbf{y}$ being too large, i.e. having too large a norm. 
Since $\mathbf{Q}$ is orthogonal, computing $\mathbf{\hat y}$ as in
equation (\ref{update}) preserves the norm of $\mathbf{y}$.  The equation
$\mathbf{\hat U^T \hat U \delta x} = \mathbf{c}$ is better conditioned
than $\mathbf{U^T U \delta x} = \mathbf{c}$ because $\lambda$ is larger.

\section{Why do this?}

The document JPL D-16159 advocates to use a conjugate-gradient method to
solve either the least-squares problem (\ref{orig}) or the normal
equations system (\ref{normal}).

When using infinite precision arithmetic, the conjugate-gradient method
converges to the same solution as a direct method after a number of
iterations equal to the number of columns in the problem.  With finite
precision arithmetic, however, the iteration may not converge at all. 
The rate of convergence can be improved by \emph{preconditioning} the
system.  The ``better'' the preconditioner, the faster the rate of
convergence.  The ideal preconditioner for the normal equations of a
least-squares problem is $\mathbf{\hat U}$.  Thus, reducing the number of
iterations of the conjugate-gradient method requires, in the limit, as
much work as solving the system by a direct method.

It is desirable to have the covariance matrix of the solution,
$(\mathbf{A^T A})^{-1} = ( \mathbf{\hat U^T \hat U} )^{-1}$, as well as
the solution.  Forming the covariance matrix of the solution by using a
conjugate-gradient method requires solving as many additional equations
as there are columns in the problem.

The matrix $\mathbf{A}$ is sparse -- it has a block-banded structure, and
a block border.  By developing a block-Cholesky solver, it is easy to
exploit the sparsity to avoid unnecessary computations.

Taken all together, the Cholesky method is preferred to the
conjugate-gradient method.

\label{lastpage}
\end{document}
% $Id$
