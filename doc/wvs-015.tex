\documentclass[12pt]{article}

\usepackage[fleqn]{amsmath}

\textwidth 6.25in
\oddsidemargin -0.25in
%\evensidemargin -0.5in
\topmargin -0.5in
\textheight 9.25in

\begin{document}

\makeatletter
\def\@biblabel#1{#1.}
\newcommand{\ps@twolines}{%
  \renewcommand{\@oddhead}{%
    15 January 2003\hfill Page \thepage\ of \pageref{lastpage}\hfill{\bf
    wvs-015}}%
\renewcommand{\@evenhead}{}%
\renewcommand{\@oddfoot}{}%
\renewcommand{\@evenfoot}{}%
}%
\makeatother
\pagestyle{twolines}

\vspace{-20pt}
\begin{tabbing}
\phantom{References: }\= \\
To: \>Michael, Bill\\
Subject: \>Exponential of a matrix, and derivative thereof\\
From: \>Van Snyder\\
\end{tabbing}

\parindent 0pt \parskip 3pt
\vspace{-20pt}

This memo describes how to compute the exponential function of a $2 \times 2$
matrix, and the derivative thereof, without encountering problems as the
eigenvalues approach each other.

One can apply the Hamilton-Cayley theorem (``A matrix satisfies its
characteristic equation'') to the series representation of any function of a
matrix to develop a formula known as ``Sylvester's Identity.''  In the case of
a $2 \times 2$ matrix {\bf A}, having eigenvalues $\lambda_1$ and $\lambda_2$,
we have

\begin{equation}
F(\mathbf{A}) = \frac{F(\lambda_1)(\mathbf{A}-\lambda_2\mathbf{I})}
                        {\lambda_1-\lambda_2} +
                   \frac{F(\lambda_2)(\mathbf{A}-\lambda_1\mathbf{I})}
                        {\lambda_2-\lambda_1} \;.
\end{equation}

When written in this form, there is clearly a computational problem as
$\lambda_1 \rightarrow \lambda_2$.

In our case $F = \exp$.  Using
 $s = \frac12 ( \lambda_1 + \lambda_2 ) = \frac12 \text{tr}(\mathbf{A})$
and
 $d = \frac12 ( \lambda_1 - \lambda_2 ) = \sqrt { s^2 - \det(\mathbf{A})}
  = \sqrt h$,
this can be rewritten as

\begin{equation}
\exp(\mathbf{A}) =
  e^s \left [ 
    \frac{\sinh d}d ( \mathbf{A} - s\, \mathbf{I} ) + \cosh d\, \mathbf{I}
      \right ] \;.
\end{equation}

This form will be useful below, but for now it can be written in a somewhat
more efficiently computable form as

\begin{equation}
\exp(\mathbf{A}) = e^{\lambda_2} \left[
 \frac{e^{2d}-1}{2d} ( \mathbf{A} - \lambda_2 \mathbf{I} ) + \mathbf{I}
  \right ]
\end{equation}

which presents no computational difficulty as $d \rightarrow 0$, provided
one computes $\frac{e^{2d}-1}{2d}$ carefully.  To do this, let $z = 2d = x + i
y$ and notice that

\begin{equation}\label{expdm1}
\frac{e^z-1}z =
 \frac{\overline{z}}{|z|^2} \left[ e^{i y} ( e^x - 1 ) + e^{i y} - 1 \right ] =
 \frac{\overline{z}}{|z|^2}
  \left[ e^{i y} ( e^x - 1 ) + \cos y - 1 + i \sin y \right ] \;.
\end{equation}

The reason to arrange the computation in this way is that both $e^x-1$ and
$\cos y -1$ can be computed without cancellation, even when $x$ and $y$ are
small, if care is taken (write a few terms of their Taylor series and you'll
agree).

To evaluate the derivative of $\exp(\mathbf{A})$, assume that $\mathbf{A}$ is a
function of $p$ and let $X^\prime$ denote $\frac{\text{d}X}{\text{d}p}$. Then
$s^\prime = \frac12 \text{tr}(\mathbf{A})^\prime$, $d^\prime = \frac{2 s
s^\prime - \det(\mathbf{A})^\prime}{2 d} = \frac{h^\prime}{2 d}$ and

\begin{equation}\begin{split}
\frac{\text{d} \exp(\mathbf{A})}{\text{d}p} =\;
 e^s & \left\{ s^\prime \left [ 
    \frac{\sinh d}d ( \mathbf{A} - s\, \mathbf{I} ) + \cosh d\, \mathbf{I}
      \right ] + \right.\\
   & \frac{h^\prime}2 \left [ 
    \frac{d \cosh d - \sinh d}{d^3} ( \mathbf{A} - s\, \mathbf{I} )
      + \frac{\sinh d}d\, \mathbf{I} \right ] +\\
   &\left.
    \frac{\sinh d}d ( \mathbf{A}^\prime - s^\prime\, \mathbf{I} )
    \right\} \;.
\end{split}\end{equation}

Collecting terms, we have

\begin{equation}\begin{split}\label{dexp}
\frac{\text{d} \exp(\mathbf{A})}{\text{d}p} =\;
 e^s & \left\{ \frac{\sinh d}d
       \left[ s^\prime \mathbf{A} + \mathbf{A}^\prime +
              \left(\frac{h^\prime}2 - s^\prime s\right) \mathbf{I} \right]
               + \right.\\
     & \left. \frac{d \cosh d - \sinh d}{d^3}
         \left[ \frac{h^\prime}{2} ( \mathbf{A} - s\, \mathbf{I} )
       + h s^\prime\, \mathbf{I} \right]
     \right \} \; \text{, or}
\end{split}\end{equation}

\begin{equation}\begin{split}
\frac{\text{d} \exp(\mathbf{A})}{\text{d}p} =\;&
  e^s \left [ s^\prime \frac{\sinh d}d +
          \frac{h^\prime}{2} \frac{d \cosh d - \sinh d}{d^3} \right ] \mathbf{A}
  + e^s \frac{\sinh d}d \mathbf{A}^\prime + \\
&   e^s \left[ \frac{\sinh d}d
               \left( \frac{h^\prime}{2} - s^\prime s \right) +
               \frac{d \cosh d - \sinh d}{d^3}
               \left( h s^\prime - \frac{h^\prime}{2} s \right) \right]
        \mathbf{I}
\end{split}\end{equation}

To see that the terms involving $\sinh d$ and $\cosh d$ present no
computational problems, write their Taylor series, \emph{viz.}
$\frac{\sinh\,d}d = \sum_{k=0}^{\infty}
\frac{d^{2k}}{(2k+1)!}$ and
%
$\frac{d \cosh d - \sinh d}{d^3} =
\sum_{k=0}^{\infty} \frac{d^{2k}}{2^k k! (2k+3)!!}$, where $(2k+3)!! =
3 \cdot 5 \cdot 7 \cdot \cdot \cdot 2k+3$.

\vskip 5pt

As the eigenvalues coalesce, no cancellations occur, and no infinities arise if
the elements of $\mathbf{A}$ and $\mathbf{A}^\prime$ are finite.  In our case,
$\mathbf{A}$ is a linear combination of three matrices, where the coefficients
are Fadeeva functions in the first quadrant (therefore $<1$ in magnitude), and
the elements of the matrices are like $\sin \phi \cos \phi \sin^2 \theta \pm i
\cos \theta$ (therefore $< \sqrt2$ in magnitude).  So the elements of
$\mathbf{A}$ are $< 3\,\sqrt2$ in magnitude, and the elements of
$\mathbf{A}^\prime$ are finite.

We can write $\frac{\sinh d}d$ as $e^{-d}\,\frac{e^{2d}-1}{2d}$, and equation
(\ref{expdm1}) shows how to do that for complex $d$.  The series for $\frac{d
\cosh d - \sinh d}{d^3}$ converges quite rapidly for small $d$, and there is
no problem to compute it as written for large $d$.

As a matter of curiosity we have

\begin{equation}
\lim_{d \rightarrow 0}
\frac{\text{d} \exp(\mathbf{A})}{\text{d}p} =
 e^s \left[
  (s^\prime+\frac{h^\prime}6)(\mathbf{A}-s\,\mathbf{I}) + \mathbf{A}^\prime +
  \frac{h^\prime}2\mathbf{I}
  \right]\;.
\end{equation}

There is cancellation in the $\mathbf{A}-s\,\mathbf{I}$ term if {\bf A} is
diagonal and $d \approx 0$, but this will be negligible compared to
$\mathbf{I}$ unless the eigenvalues are as large as the inverse of the
round-off level, which they aren't in our case.  We couldn't compute near that
condition anyway, because we can only evaluate $e^s$ if $s$ is less than the
logarithm of the overflow limit.

\label{lastpage}
\end{document}
% $Id$
