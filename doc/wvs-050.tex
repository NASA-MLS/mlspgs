\documentclass[landscape]{beamer}
\usepackage[nofancy,notoday]{rcsinfo}
% \usepackage{pstcol,pst-plot,pst-char,pst-node}
\usepackage{color}
\usepackage[final]{epsfig}
\usepackage{amsmath}

\usepackage{graphicx}

\renewcommand{\b}{\mathbf}
\newcommand{\T}{^{T}}
\newcommand{\cs}[1]{$_{\text{#1}}$}
\newcommand{\inv}{^{\mathrm -1}}
\newcommand{\cp}[1]{$^{\text{#1}}$}
\newcommand{\degsym}{\ensuremath{^\circ}}
\newcommand{\newframe}[2][]{\begin{frame}\frametitle{\hfill #2 \hfil}}

\hypersetup{pdfborder={0 0 0}}
%
% -----------------------------------------------------------------------------
%
\title{Summary of Retriever}
\subtitle{wvs-050r1}
\author{Van Snyder}
\date{28 May 2019} 
\titlegraphic{\includegraphics[width=1.0in]{eos_mls_logo_onpink}}
\begin{document}
\sloppy

%
% -----------------------------------------------------------------------------
%
\begin{frame}
 \titlepage
\end{frame}

% \maketitle
%
% -----------------------------------------------------------------------------
%
\newframe{Nomenclature for next slides}
\begin{itemize}
\item $i$: MAF number
\item $\b{C}$: Column scaling matrix
\item $\b{y}_i-\b{\hat y}_i$: (Measured radiances) - (calculated radiances)
  for the $i^{\text{th}}$ MAF
\item $\b{x}_n$: $n^{\text{th}}$: estimate of the state; $\delta\b{x} =
  \b{x}_{n+1} - \b{x}_n$; $\delta\b{\tilde x} = \b{C}^{-1} \delta \b{x}$
\item $\b{K}$: The entire matrix of the least-squares problem; $\b{\tilde K}
  = \b{K C}$
\item $\b{J}_i$: The Jacobian matrix of derivatives for the
  $i^{\text{th}}$ MAF, $\frac{\partial \b{f}_i(\b{x}_n)}{\partial\b{x}_n}$
\item $\b{F}$: Cholesky factor of $\b{S}_a^{-1}$ (a priori covariance),
  i.e.\ $\b{F}^T\b{F} = \b{S}_a^{-1}$
\item $\b{W}_i$: Weighting matrix for rows of $i^{\text{th}}$
  MAF = Cholesky factor of $\b{S}_{{\epsilon}_i}^{-1}$ (measurement
  covariance for the $i^{\text{th}}$ MAF), i.e.\ $\b{W}_i^T \b{W}_i =
  \b{S}_{{\epsilon}_i}^{-1}$
\item $\lambda$: Levenberg-Marquardt stabilization parameter
\item $\b{R}$: Regularization matrix -- a difference operator
\item $\mu_a$, $\mu_R$: Scalar ``intensity'' parameters
\item $-\b{f}$: Entire right-hand-side vector of the least-squares problem
\end{itemize}
\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Introduction to Newton iteration}
For a scalar function, say $f$, of a scalar argument, say $x$, the Taylor series
truncated to first order is
\begin{equation}
f(x) \approx f(x_0) + (x-x_0) f^{\,\prime}(x0)
\end{equation}
The Newton iteration for finding a zero of $f$ is gotten by setting $f(x)$
to zero and solving for $x$.  It is usually written
\begin{equation}
  x_{n+1} = x_n - \frac{f(x_n)}{f^{\,\prime}(x_n)}\;,
\end{equation}
where $n$ is initially zero.  Rearranging terms, we have
\begin{equation}\label{Newton}
  f^{\,\prime}(x_n) ( x_{n+1} - x_n ) = f^\prime(x_n) \,\delta x = -
  f(x_n)\;.
\end{equation}
\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Least-squares view of problem (1)}
In the case of a vector function of a vector argument, the derivative
$f^{\,\prime}(x_n)$ is replaced by the Jacobian matrix $\b{J} =
\frac{\partial \b{f}(\b{x}_n)}{\partial \b{x}_n}$, so equation
(\ref{Newton}) becomes $J\,\delta \b{x} = -\b{f}(\b{x}_n)$.

When one has many more observations (values of $\b{f}$) than states for
which to solve (values of $\b{x}_n$), one has a least-squares problem to
solve at each iteration, instead of an equation.

If the least-squares problem is poorly conditioned, as it is in our case,
one needs to add extra constraints to make a solution tractable.
\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Least-squares view of problem (2)}
In broad outline, the least-squares problem to be solved in each Newton
iteration consists of four parts:
\begin{equation}\label{four}
 \begin{split}
   \b{J} \delta \b{x}         &\simeq - \b{f} \\
   \b{F} \b{x}_{n+1}          &\simeq \b{F\, a} \\
   \b{R} \b{x}_{n+1}          &\simeq 0 \\
   \lambda \b{I} \delta \b{x} &\simeq 0
  \end{split}
\end{equation}
\begin{itemize}
\item The first part you know already
\item The second part is ``The solution should approximate the a~priori''
\item The third part is ``The solution has a particular functional form''
  (this is called \emph{regularization})
\item The fourth part is ``The Jacobian is a good linear approximation on
  a scale given by $\lambda^{-1}$'' (this is known as
  \emph{Levenberg-Marquardt stabilization}).
\end{itemize}
\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Least-squares view of problem (3)}

Equations (\ref{four}) are not all equations to solve for
$\delta\b{x}$.  Re-writing the second and third relations, we have

\begin{equation*}
 \begin{split}
   \b{J}\, \delta \b{x}                  &\simeq - \b{f} \\
   \b{F}\, \b{x}_{n+1} - \b{F}\, \b{x}_n &\simeq \b{F\, a} - \b{F}\, \b{x}_n \\
   \b{R}\, \b{x}_{n+1} - \b{R}\, \b{x}_n &\simeq - \b{R}\, \b{x}_n \\
   \lambda \b{I}\, \delta\, \b{x}        &\simeq 0
  \end{split}
\end{equation*}

or

\begin{equation*}
\left[ \begin{array}{l}
  \b{J} \\
  \b{F} \\
  \b{R} \\
  \lambda \b{I}\\
\end{array} \right] \delta \b{x}
\simeq
\left[
\begin{array}{l}
  - \b{f} \\
  \b{F}\, ( \b{a} - \b{x}_n) \\
  - \b{R}\, \b{x}_n \\
  0\\
\end{array} \right]
\end{equation*}

Putting these all together, weighting them, scaling, ... we have ...
\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Least-squares view of problem (4)}

The Newton iteration consists of evaluating $\b{K}$ and $\b{f}$ at
$\b{x}_n$, solving $\b{\tilde K}\delta\b{ \tilde x} \simeq -\b{f}$ to get
$\b{x}_{n+1}$, and repeating until $|\b{f}|$ or $|\delta\b{x}|$ is ``small
enough''.  In detail:

\begin{equation*}
\b{\tilde K} \delta \b{\tilde x} = \b{K} \b{C} \; \b{C}^{-1} \delta\b{x} =
 \left [ \begin{array}{c} \text{...} \\
                          \b{W}_i \b{J}_i \\
                          \text{...} \\
                          \mu_a \b{F} \\
                          \mu_R \b{R} \\
                          \lambda \b{I}
 \end{array} \right ]
 \b{C} \; \b{C}^{-1} \delta\b{x} \simeq
 \left [ \begin{array}{c} \text{...} \\
                          \b{W}_i \, ( \b{y}_i - \b{\hat y}_i ) \\
                          \text{...} \\
                          \mu_a \b{F}\, ( \b{a} - \b{x}_n ) \\
                          -\mu_R \b{R} \, \b{x}_n \\
                          \b{0}
 \end{array} \right ] = -\b{f}
\end{equation*}

where $\b{\tilde K} = \b{K\,C}$ and $\delta \b{\tilde x} = \b{C}^{-1}
\delta \b{x}$.

\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Least-squares view of problem (5)}
After multiplying by $\b{\tilde K}^T$ to form normal equations, this becomes:
\begin{equation*}
\begin{split}
%\begin{array}{ll}
  \b{\tilde K}^T \b{\tilde K} \delta\b{\tilde x} &= -\b{\tilde K}^T \b{f} =
  \b{C}^T \b{K}^T \b{K} \b{C} \; \b{C}^{-1} \delta \b{x} = \\
   &= \b{C}^T \left [ \Sigma_i \b{J}_i^T \b{S}_{{\epsilon}_i}^{-1} \b{J}_i +
   \mu_a^2 \b{S}_a^{-1} + \mu_R^2 \b{R}^T \b{R} + \lambda^2 \b{I} \right ] \b{C} \,
   \b{C}^{-1} \delta \b{x}= \\
  &= -\b{C}^T \b{K}^T \b{f} =\\
  &= \b{C}^T \left [ \Sigma_i \b{J}_i^T \b{S}_{{\epsilon}_i}^{-1}
     (\b{y}_i-\b{\hat y}_i) + \mu_a^2 \b{S}_a^{-1} ( \b{a}-\b{x}_n ) -
     \mu_R^2 \b{R}^T \b{R} \b{x}_n
   \right ]
\end{split}
%\end{array}
\end{equation*}
where $\b{C}$ is chosen either to minimize the condition number of
$\b{\tilde K}^T \b{\tilde K}$ or to make the expected errors in
$\delta\b{x}$ roughly the same.

Finally

\begin{equation*}
\delta \b{x} = \b{C} \, \delta \b{\tilde x} \,.
\end{equation*}

\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe[Solving]{Solving the least-squares problem (1)}
The retriever uses Cholesky factorization to solve the least-squares problem. 
It has the advantages that it is guaranteed to produce an answer if the normal
equations are non-singular, it doesn't need pivoting to maintain stability, and
it is easy to take advantage of our sparsity pattern.  It has the disadvantage
of needing to form the normal equations, which squares the condition
number.
%
Alternatives are
\begin{itemize}
\item Preconditioned conjugate gradient, which has the advantage that it may
  converge after a few matrix-vector multiplies, but has the disadvantages that
  it may not converge at all if finite arithmetic is used, and calculating
  the covariance matrix doubles the work.
\end{itemize}

\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe[Solving]{Solving the least-squares problem (2)}
\begin{itemize}
\item Householder triangularization, which has the advantages that it works
  directly on the least-squares problem (i.e.\ doesn't need to form normal
  equations) and therefore doesn't square the condition number, but it
  can nonetheless be organized to accumulate the factored problem
  little-by-little (so it doesn't need to have the whole of the $\b{K}$
  matrix at any one time), and sparsity can be exploited.  It has the
  disadvantage of requiring about twice as much work as forming normal
  equations followed by Cholesky factorization.
\end{itemize}
\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Accumulating normal equations little-by-little}

\small
\begin{itemize}
\item In the left-hand side of the normal equations $\b{\tilde K}^T
  \b{\tilde K} \delta \b{\tilde x} = -\b{\tilde K}^T \b{f}$, the matrix
  $\b{N} = \b{\tilde K}^T \b{\tilde K}$ is formed by computing the inner
  product of each column of $\b{\tilde K}$ with every other column.  That
  is, $\b{N}_{ij} = \Sigma_{k=1}^m \b{\tilde K}_{ik} \b{\tilde K}_{jk}$

\item Accumulating normal equations little-by-little amounts to nothing
  more than running the summation for $k$ from $1$ to $m_1$ for all $i$
  and $j$, then from $m_1+1$ to $m_2$ for all $i$ and $j$, etc., i.e.\
  changing the order of addition.

\item A similar perspective applies in forming the right-hand-side vector
  $-\b{\tilde K}^T \b{f}$ little-by-little.

\item Accumulating normal equations little-by-little allows to compute a
  piece of the Jacobian matrix, accumulate it into the normal equations,
  and discard it.  This reduces our storage requirements by a factor of
  10~-- from 10--40~GB to less than 4~GB.

\item Since the Jacobian matrix has more than twice as many rows as
  columns (actually, typically 100 times more rows than columns), we
  scale the rows and columns of the normal equations instead of column
  scaling the Jacobian.
\end{itemize}

\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe[Algebra]{Organization of the linear algebra software (1)}
\begin{itemize}
\item The Jacobian matrix is sparse -- mostly zeroes -- and we know the
  pattern of its sparsity -- roughly diagonal plus a block column on the
  right.  The normal equations are therefore also sparse with a known
  pattern.

\item Exploiting the sparsity and its pattern leads to significant
  reduction in computational effort and memory requirements.

\item To exploit the sparsity, the software to implement the linear
  algebra is divided into two levels.

\item The high level software performs matrix--matrix and matrix--vector
  operations using submatrix blocks.
\end{itemize}
\end{frame}
%
% ----------------------------------------------------------------------------
%
\newframe{Organization of the linear algebra software (2)}

The low level software performs operations on the matrix blocks using one
of four representations:

\begin{description}
\item[Absent] blocks consisting entirely of zeroes
\item[Banded] blocks in which the nonzeroes in each column are a small
fraction of the column, and are adjacent to each other
\item[Sparse] blocks in which the nonzeroes in each column are a small
fraction of the column, but are not adacent to each other
\item[Full] blocks in which the nonzeroes are a significant fraction of
the whole.
\end{description}

The forward models usually know which representation to use.  Just in
case, a procedure that takes a full block as input, and produces the
appropriate representation as output, is provided.
\end{frame}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

% $Log$
% Revision 1.2  2019/05/22 20:40:03  vsnyder
% Convert from slides class to beamer class
%
% Revision 1.1  2008/06/11 20:14:53  vsnyder
% Initial commit
%
% Revision 1.1  2008/06/11 20:14:53  vsnyder
% Initial commit
%
% Revision 1.7  2001/06/16 00:57:56  vsnyder
% More stuff about least-squares, some clean-up
%
% $Id$
