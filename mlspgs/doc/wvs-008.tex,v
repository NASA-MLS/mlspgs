head	1.1;
access;
symbols
	v5-02-NRT-19:1.1
	v6-00:1.1
	v5-02-NRT-18:1.1
	v5-02:1.1
	v5-01-NRT-17:1.1
	v5-01-NRT-16:1.1
	v5-01-NRT-15:1.1
	v5-01-NRT-14:1.1
	neuralnetworks-1-0:1.1.0.14
	cfm-single-freq-0-1:1.1.0.12
	v5-01:1.1
	v5-00:1.1
	v4-23-TA133:1.1.0.10
	mus-emls-1-70:1.1.0.8
	rel-1-0-englocks-work:1.1.0.6
	VUMLS1-00:1.1
	VPL1-00:1.1
	V4-22-NRT-08:1.1
	VAM1-00:1.1
	V4-21:1.1.0.4
	V4-13:1.1
	V4-12:1.1
	V4-11:1.1
	V4-10:1.1
	V3-43:1.1
	M4-00:1.1
	V3-41:1.1
	V3-40-PlusGM57:1.1.0.2
	V3-33:1.1
	V3-31:1.1
	V3-30-NRT-05:1.1
	cfm-01-00:1.1
	V3-30:1.1
	V3-20:1.1
	V3-10:1.1;
locks; strict;
comment	@% @;


1.1
date	2008.06.11.20.14.50;	author vsnyder;	state Exp;
branches;
next	;


desc
@@


1.1
log
@Initial commit
@
text
@\documentclass[11pt]{article}
\usepackage[fleqn]{amsmath}\textwidth 6.25in
\oddsidemargin -0.25in
%\evensidemargin -0.5in
\topmargin -0.5in
\textheight 9.0in

\begin{document}

%\tracingcommands=1
\newlength{\hW} % heading box width
%\settowidth{\hW}{\bf wvs-008}
\settowidth{\hW}{Page \pageref{lastpage}\ of \pageref{lastpage}}
\makeatletter
\def\@@biblabel#1{#1.}
\newcommand{\ps@@twolines}{%
  \renewcommand{\@@oddhead}{%
    11 September 2000\hfill\parbox[t]{\hW}{{\bf wvs-008}\newline
                          Page \thepage\ of \pageref{lastpage}}}%
\renewcommand{\@@evenhead}{}%
\renewcommand{\@@oddfoot}{}%
\renewcommand{\@@evenfoot}{}%
}%
\makeatother
\pagestyle{twolines}

\vspace{-10pt}
\begin{tabbing}
\phantom{References: }\= \\
To: \>Dave, Nathaniel\\
Subject: \>Summary of work I asked Fred Krogh to do recently\\
From: \>Van Snyder\\
\end{tabbing}

\parindent 0pt \parskip 3pt
\vspace{-20pt}

\section{Introduction}

In developing the Matrix module, it appeared to me that the best
representation for matrices that are not full is to represent the
columns. The most interesting case for our application is for a column in
which the nonzeros are consecutive, and constitute a small fraction of
the column, i.e. the band-limited case.

We have decided to approach the linear least-squares problem within the
nonlinear problem by forming normal equations, and solving them by using
Cholesky decomposition.  If we let the composite of all the matrices be
$A$, the normal equations correspoinding to the least-squares problem $A
x \sim b$ are $A^T A x \sim A^T b$.  It is clear that these equations can
be formed by performing column operations on $A$.  It was less obvious to
me that having formed the normal equations, one could compute their
Cholesky factor by using column operations alone.

This and other considerations led me to ask Fred Krogh to investigate
four questions.  Here are the questions and his brief answers.  Following,
there is a superficial discussion of the reasoning behind the answers.

\begin{enumerate}
\item Q: Assuming $A$ is banded, what is the bandheight of $A^T A$ vs.
      $A$?\\ A: The bandheight of $A^T A$ is one less than twice the
      bandheight of $A$.
\item Q: If just the lower triangle of $A^T A$ is saved can we get by
         doing only column operations to compute the Cholesky factor $L$
         of $A^T A$?\\
      A: Yes.
\item Q: Can we form the Cholesky factor $L$ of $A^T A$ at the same time as
         we form $A^T A$?\\
      A: Yes.
\item Q: Does $L$ require the same space as the lower triangle of $A^T A$?\\
      A: Unfortunately no.  The answer is similar to the answer for question
         1, i.e. slightly less than twice as much space is required for
         $L$.
\end{enumerate}

\section{Brief discussions}

\begin{enumerate}

\item Let $C = A^T A$, let $a_i$ denote the $i^{\text{th}}$ column of
$A$, and let $c_{i,j}$ denote an element of $C$.  Then $c_{i,j} = a_i^T
a_j$, i.e. it is the inner product of columns $i$ and $j$.  Draw some
pictures of how such inner products intersect and the result should be
obvious.

\item This is the complicated one to explain (but easy to implement). 
Let $L L^T = C$, and denote the elements of $L$ by $\ell_{i,j}$.  The
rules for matrix multiplication can be used to derive

$\ell_{i,i} = ( c_{i,i} - \sum_{k=1}^{i-1} \ell_{k,k} )^{\frac12}$ and for $i >
j$ $\ell_{i,j} = ( c_{i,j} - \sum_{k=1}^{j-1} \ell_{i,k} \ell_{j,k} ) / \ell_{j,j}$.

It is the $\ell_{i,j}$ case that is of interest.  (Computations are done for
$j = 1, 2, 3, ..., n$ and then for each $j$ for $i = j, j+1, ... n$.)

One obtains column operations by forming a partial sum for $\ell_{i,j}$ for
all applicable $i$'s for $k=1$, then updating the sum for $k=2, ...$ . 
This appears to be entirely straightforward.  Note that since $i \geq j$,
we only compute the nonzero entries in $L$, and only use the lower
triangle of $C$.

It should be noted that making the most effective use of cache may
require these operations to be blocked.  Something that I would suggest
doing after getting the more straightforward version going.

\item Clearly from the above, once we have the $j^{\text{th}}$ column of
$C$, we can form the $j^{\text{th}}$ column of $L$.  Once again issues of
making effective use of cache may come into play which would favor an
algorithm which does not work in this way.

\item Terms in the sum will be zero for all $k$ sufficiently small such
that $\ell_{i,k}$ is 0, or for all $k$ sufficiently large that $\ell_{j,k}$ is
0.  This is much like the kind of rule for item 1, and with the same
result.  Try some (very) small examples and you will see the pattern.

\end{enumerate}

\label{lastpage}
\end{document}
% $Id: $
@
