head	1.1;
access;
symbols
	v5-02-NRT-19:1.1
	v6-00:1.1
	v5-02-NRT-18:1.1
	v5-02:1.1
	v5-01-NRT-17:1.1
	v5-01-NRT-16:1.1
	v5-01-NRT-15:1.1
	v5-01-NRT-14:1.1
	neuralnetworks-1-0:1.1.0.14
	cfm-single-freq-0-1:1.1.0.12
	v5-01:1.1
	v5-00:1.1
	v4-23-TA133:1.1.0.10
	mus-emls-1-70:1.1.0.8
	rel-1-0-englocks-work:1.1.0.6
	VUMLS1-00:1.1
	VPL1-00:1.1
	V4-22-NRT-08:1.1
	VAM1-00:1.1
	V4-21:1.1.0.4
	V4-13:1.1
	V4-12:1.1
	V4-11:1.1
	V4-10:1.1
	V3-43:1.1
	M4-00:1.1
	V3-41:1.1
	V3-40-PlusGM57:1.1.0.2
	V3-33:1.1
	V3-31:1.1
	V3-30-NRT-05:1.1
	cfm-01-00:1.1
	V3-30:1.1
	V3-20:1.1
	V3-10:1.1;
locks; strict;
comment	@% @;


1.1
date	2008.06.11.20.14.50;	author vsnyder;	state Exp;
branches;
next	;


desc
@@


1.1
log
@Initial commit
@
text
@\documentclass[11pt]{article}
\textwidth 6.25in
\oddsidemargin -0.25in
%\evensidemargin -0.5in
\topmargin -0.5in
\textheight 9.25in

\begin{document}

%\tracingcommands=1
\makeatletter
\def\@@biblabel#1{#1.}
\newcommand{\ps@@twolines}{%
  \renewcommand{\@@oddhead}{%
    28 June 2001\hfill Page \thepage\ of \pageref{lastpage}\hfill{\bf
    wvs-014}}%
\renewcommand{\@@evenhead}{}%
\renewcommand{\@@oddfoot}{}%
\renewcommand{\@@evenfoot}{}%
}%
\makeatother
\pagestyle{twolines}

\vspace{-20pt}
\begin{tabbing}
\phantom{References: }\= \\
To: \>Dave, Nathaniel\\
Subject: \>More remarks on solving the linear system during retrieval\\
From: \>Van Snyder\\
References: \>wvs-011
\end{tabbing}

\parindent 0pt \parskip 5pt

Consider the least-squares problem ${\bf Ax} = {\bf b+r}$, where $|| {\bf
r} ||_2$ is minimal at the solution.  This is frequently written as 
${\bf A x} \simeq {\bf b}$.  It can be shown that a condition for $||
{\bf r} ||_2$ to be minimal at the solution is that it is not in the
column space of ${\bf A}$, i.e. ${\bf A^T r} = {\bf 0}$.  (This
demonstration is beyond the scope of this memorandum.)

A method was shown in wvs-011 to compute the quantity $r^2 = {\bf
b^T}({\bf b}-{\bf Ax})$, which is of interest to the nonlinear solver,
without keeping ${\bf A}$ until ${\bf x}$ is computed.  That method
introduced substantial complication into the software.  An alternative
formulation is presented here.

Using ${\bf Ax = b+r}$, we have ${\bf b^T} ({\bf b - Ax}) = -{\bf b^Tr}$.
We also have ${\bf x^T A^T A x} = ({\bf b+r})^{\bf T}( {\bf b+r}) = {\bf
b^T b} + 2{\bf b^T r} + {\bf r^T r}$ and ${\bf x^T A^T A x} =  {\bf x^T
A^T} ( {\bf b+r} ) = {\bf x^T A^T b}$ (remember that ${\bf A^T r} = {\bf
0}$).  Expanding the last result, we have ${\bf x^T A^T A x} = {\bf b^T b
+ b^T r}$.  Equating these two expansions of ${\bf x^T A^T A x}$, we have
$-{\bf b^T r} = {\bf r^T r}  = r^2$.

To solve ${\bf A x} \simeq {\bf b}$ exploit ${\bf A^T r} = {\bf 0}$
giving ${\bf A^T A x} = {\bf A^T b}$ (this is called the \emph{normal
equations} system of the least-squares problem).  Let ${\bf U}$ be the
Cholesky factor of ${\bf A^T A}$, i.e. ${\bf U^T U} = {\bf A^T A}$ and
${\bf U}$ is upper triangular. Then we can solve ${\bf A^T A x} = {\bf
A^T b}$ in two steps by letting ${\bf y} = {\bf U x}$, solving ${\bf U^T
y} = {\bf A^T b}$ for ${\bf y}$, and then solving ${\bf U x} = {\bf y}$
for ${\bf x}$.

We can therefore compute $r$ at low cost by noticing that  $r^2 = {\bf
r^T r} = {\bf b^T b} - {\bf x^T A^T A x} = {\bf b^T b} - {\bf x^T U^T U
x} = {\bf b^T b} - {\bf y^T y}$.  I.e., we do not need to keep ${\bf A}$
until ${\bf x}$ is computed in order to compute $r$.  The final
consideration is the entire point of this exercise:  We form the normal
equations little by little, and discard parts of ${\bf A}$ as they are
incorporated into ${\bf A^T A x} = {\bf A^T b}$.  If we needed to keep
${\bf A}$ until ${\bf x}$ was computed, it would increase our memory
requirements about ten fold.

\label{lastpage}
\end{document}
% $Id: $
@
